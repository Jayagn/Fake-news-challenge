{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import plot_model \n",
    "from IPython.display import Image\n",
    "import pydot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "#from keras import initializations\n",
    "from keras import initializers, regularizers, constraints\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from string import punctuation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Activation, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Jayagn/Fake-news-challenge.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the folder locations\n",
    "GloVe_DIR = './data/glove.twitter.27B.50d.txt'\n",
    "DATA_DIR = './data'\n",
    "MAX_SENT_LEN = 150 #to be tested on (150, 300 and 700)\n",
    "MAX_VOCAB_SIZE = 28000 #vocabulary\n",
    "BATCH_SIZE = 512\n",
    "N_EPOCHS = 20 \n",
    "LSTM_DIM = 50\n",
    "EMBEDDING_DIM = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text files of fnc data\n",
    "bodies = pd.read_csv(DATA_DIR + '/body_id.csv')\n",
    "train_df = pd.read_csv(DATA_DIR + '/train.csv')\n",
    "#validation_df = pd.read_csv(DATA_DIR + '/validation_data.csv')\n",
    "test_df = pd.read_csv(DATA_DIR + '/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.replace('agree',2,True)\n",
    "train_df.replace('disagree',3,True)\n",
    "train_df.replace('discuss',4,True)\n",
    "combine_df_train = train_df.join(bodies.set_index('Body ID'), on='Body ID')\n",
    "combine_df_test = test_df.join(bodies.set_index('Body ID'), on='Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing the data\n",
    "word_seq_head_train = [text_to_word_sequence(head) for head in combine_df_train['Headline']]\n",
    "word_seq_bodies_train = [text_to_word_sequence(body) for body in combine_df_train['articleBody']]\n",
    "word_seq_head_test = [text_to_word_sequence(head) for head in combine_df_test['Headline']]\n",
    "word_seq_bodies_test = [text_to_word_sequence(body) for body in combine_df_test['articleBody']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_seq = []\n",
    "for i in range(len(word_seq_head_train)):\n",
    "    word_seq.append(word_seq_head_train[i])\n",
    "for i in range(len(word_seq_bodies_train)):\n",
    "    word_seq.append(word_seq_bodies_train[i])\n",
    "for i in range(len(word_seq_head_test)):\n",
    "    word_seq.append(word_seq_head_test[i])\n",
    "for i in range(len(word_seq_bodies_test)):\n",
    "    word_seq.append(word_seq_bodies_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing sentences\n",
    "filter_list = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters=filter_list)\n",
    "tokenizer.fit_on_texts([seq for seq in word_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining headline and body together\n",
    "word_seq_train = [list(i) for i in word_seq_head_train]\n",
    "for i in range(len(word_seq_head_train)):\n",
    "    word_seq_train[i].extend(word_seq_bodies_train[i]) \n",
    "    \n",
    "word_seq_test = [list(i) for i in word_seq_head_test]\n",
    "for i in range(len(word_seq_head_test)):\n",
    "    word_seq_test[i].extend(word_seq_bodies_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding the data\n",
    "X_train = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq_train])\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "y_train = combine_df_train['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the sequence of words to sequnce of indices\n",
    "X_test = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq_test])\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SENT_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding\n",
    "encoder_train = LabelEncoder()\n",
    "encoder_train.fit(y_train)\n",
    "encoded_train = encoder_train.transform(y_train)\n",
    "dummy_y_train = np_utils.to_categorical(encoded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_vali, y_train, y_vali = train_test_split(X_train, dummy_y_train, random_state=10, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GloVes embedding\n",
    "glove_input_file = GloVe_DIR\n",
    "word2vec_output_file = 'glove.50d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an embedding matrix containing only the word's in our vocabulary\n",
    "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items(): \n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "    except KeyError:\n",
    "        embeddings_vector = None\n",
    "\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i] = embeddings_vector\n",
    "        \n",
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only LSTM\n",
    "#Build a sequential model by stacking neural net units\n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=EMBEDDING_DIM,\n",
    "                          weights = [embeddings_matrix], trainable=False, name='word_embedding_layer', \n",
    "                          mask_zero=True)) # trainable=True results in overfitting\n",
    "\n",
    "model_2.add(LSTM(LSTM_DIM, return_sequences=False, name='lstm_layer'))\n",
    "model_2.add(Activation(activation='relu', name='activation_1'))\n",
    "model_2.add(Dense(4, activation='softmax', name='output_layer'))\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_2, to_file='lstm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(lr=0.001) #Tried varying learning rate\n",
    "model_2.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2 = model_2.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=20,validation_data=(X_vali, y_vali))\n",
    "model_2.save('lstm_700.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_2.history['accuracy'])\n",
    "plt.plot(history_2.history['val_accuracy'])\n",
    "plt.title('Model accuracy with only LSTM(Truncation: 300 Epoch: 20)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2_df = pd.DataFrame(history_2.history)\n",
    "history_2_df.to_csv(\"history_2_700.csv\",header=True,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with BatchNormalization and Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM Model with BatchNormalization and Dropouts\n",
    "#Build a sequential model by stacking neural net units \n",
    "#dense layer: simply a layer connect units \n",
    "#dropout layer: for reduce overfitting a regularization method\n",
    "model_3 = Sequential()\n",
    "model_3.add(Embedding(input_dim=len(tokenizer.word_index)+1,output_dim=EMBEDDING_DIM, weights = [embeddings_matrix], trainable=False, name='word_embedding_layer', \n",
    "                          mask_zero=True)) # trainable=True results in overfitting\n",
    "\n",
    "model_3.add(LSTM(LSTM_DIM, return_sequences=False, name='lstm_layer')) # Can try Bidirectional-LSTM\n",
    "\n",
    "#model.add(Dense(32, name='dense_1'))\n",
    "model_3.add(BatchNormalization(name='bn_1')) # BN did not really help with performance \n",
    "model_3.add(Dropout(rate=0.4, name='dropout_1')) # Can try varying dropout rates, in paper suggest 0.2\n",
    "model_3.add(Activation(activation='relu', name='activation_1'))\n",
    "\n",
    "\n",
    "#model.add(Dense(8, name='dense_2'))\n",
    "model_3.add(BatchNormalization(name='bn_2'))\n",
    "model_3.add(Dropout(rate=0.2, name='dropout_2'))\n",
    "#model_3.add(Activation(activation='relu', name='activation_2'))\n",
    "\n",
    "model_3.add(Dense(4, activation='softmax', name='output_layer'))\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_3, to_file='lstm_with_dropout.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(lr=0.001)\n",
    "model_3.compile(loss='categorical_crossentropy',optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_3 = model_3.fit(X_train, y_train, batch_size=BATCH_SIZE,epochs=20, validation_data=(X_vali, y_vali))\n",
    "model_3.save('lstm_with_dropout_700.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.save('lstm_with_dropout.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_3.history['accuracy'])\n",
    "plt.plot(history_3.history['val_accuracy'])\n",
    "plt.title('Model accuracy of only LSTM with Dropouts(Truncation: 300 Epoch: 20)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_3_df = pd.DataFrame(history_3.history)\n",
    "history_3_df.to_csv(\"history_3_700.csv\",header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
