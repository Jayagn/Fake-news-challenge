{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BosPCj5x38q_"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import plot_model \n",
    "from IPython.display import Image\n",
    "import pydot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "#from keras import initializations\n",
    "from keras import initializers, regularizers, constraints\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from string import punctuation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Activation, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JgpBdh-t04Ya",
    "outputId": "110fc998-e496-4ac5-e253-e24df291cac8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Fake-news-challenge' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Jayagn/Fake-news-challenge.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IdCx9Trp4KJf"
   },
   "outputs": [],
   "source": [
    "#Specifying the folder locations\n",
    "GloVe_DIR = './data/glove.twitter.27B.50d.txt'\n",
    "DATA_DIR = './data'\n",
    "MAX_SENT_LEN = 150 #to be tested on (150, 300 and 700)\n",
    "MAX_VOCAB_SIZE = 28000 #vocabulary\n",
    "BATCH_SIZE = 512\n",
    "N_EPOCHS = 20 \n",
    "LSTM_DIM = 50\n",
    "EMBEDDING_DIM = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R1AqH9BB4Wlz"
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TE0HAJOI4dRM"
   },
   "outputs": [],
   "source": [
    "# Read the text files of fnc data\n",
    "bodies = pd.read_csv(DATA_DIR + '/body_id.csv')\n",
    "train_df = pd.read_csv(DATA_DIR + '/train.csv')\n",
    "#validation_df = pd.read_csv(DATA_DIR + '/validation_data.csv')\n",
    "test_df = pd.read_csv(DATA_DIR + '/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTep5suA4zEg"
   },
   "outputs": [],
   "source": [
    "train_df.replace('unrelated',1,True)\n",
    "train_df.replace('agree',2,True)\n",
    "train_df.replace('disagree',3,True)\n",
    "train_df.replace('discuss',4,True)\n",
    "combine_df_train = train_df.join(bodies.set_index('Body ID'), on='Body ID')\n",
    "combine_df_test = test_df.join(bodies.set_index('Body ID'), on='Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsuyjJQJ438C"
   },
   "outputs": [],
   "source": [
    "#Pre-processing the data\n",
    "word_seq_head_train = [text_to_word_sequence(head) for head in combine_df_train['Headline']]\n",
    "word_seq_bodies_train = [text_to_word_sequence(body) for body in combine_df_train['articleBody']]\n",
    "word_seq_head_test = [text_to_word_sequence(head) for head in combine_df_test['Headline']]\n",
    "word_seq_bodies_test = [text_to_word_sequence(body) for body in combine_df_test['articleBody']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YUq6I9f46Bp"
   },
   "outputs": [],
   "source": [
    "word_seq = []\n",
    "for i in range(len(word_seq_head_train)):\n",
    "    word_seq.append(word_seq_head_train[i])\n",
    "for i in range(len(word_seq_bodies_train)):\n",
    "    word_seq.append(word_seq_bodies_train[i])\n",
    "for i in range(len(word_seq_head_test)):\n",
    "    word_seq.append(word_seq_head_test[i])\n",
    "for i in range(len(word_seq_bodies_test)):\n",
    "    word_seq.append(word_seq_bodies_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4FauKgO48wv"
   },
   "outputs": [],
   "source": [
    "#Tokenizing sentences\n",
    "filter_list = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters=filter_list)\n",
    "tokenizer.fit_on_texts([seq for seq in word_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "54h0SNB54-z1"
   },
   "outputs": [],
   "source": [
    "#Combining headline and body together\n",
    "word_seq_train = [list(i) for i in word_seq_head_train]\n",
    "for i in range(len(word_seq_head_train)):\n",
    "    word_seq_train[i].extend(word_seq_bodies_train[i]) \n",
    "    \n",
    "word_seq_test = [list(i) for i in word_seq_head_test]\n",
    "for i in range(len(word_seq_head_test)):\n",
    "    word_seq_test[i].extend(word_seq_bodies_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s7cATGCY5BaQ"
   },
   "outputs": [],
   "source": [
    "#Padding the data\n",
    "X_train = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq_train])\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "y_train = combine_df_train['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yvgtp-k25ELT"
   },
   "outputs": [],
   "source": [
    "#Converting the sequence of words to sequnce of indices\n",
    "X_test = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq_test])\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SENT_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eaODiMhP5GyJ"
   },
   "outputs": [],
   "source": [
    "#One hot encoding\n",
    "encoder_train = LabelEncoder()\n",
    "encoder_train.fit(y_train)\n",
    "encoded_train = encoder_train.transform(y_train)\n",
    "dummy_y_train = np_utils.to_categorical(encoded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HMUHqN_5JDd"
   },
   "outputs": [],
   "source": [
    "X_train, X_vali, y_train, y_vali = train_test_split(X_train, dummy_y_train, random_state=10, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "vzQ92uJQ5OWc",
    "outputId": "4aea3ec6-16b9-4307-f2fe-507e081aeb96"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "#GloVes embedding\n",
    "glove_input_file = GloVe_DIR\n",
    "word2vec_output_file = 'glove.50d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KduzB5Vw5Yn4"
   },
   "outputs": [],
   "source": [
    "#Create an embedding matrix containing only the word's in our vocabulary\n",
    "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items(): \n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "    except KeyError:\n",
    "        embeddings_vector = None\n",
    "\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i] = embeddings_vector\n",
    "        \n",
    "del embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JpbPsqX6cbOe"
   },
   "outputs": [],
   "source": [
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "        at=K.softmax(et)\n",
    "        at=K.expand_dims(at,axis=-1)\n",
    "        output=x*at\n",
    "        return K.sum(output,axis=1)\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(attention,self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BzPsZLnjGctf"
   },
   "outputs": [],
   "source": [
    "model_4 = Sequential()\n",
    "model_4.add(Embedding(input_dim=len(tokenizer.word_index)+1, input_length = MAX_SENT_LEN,\n",
    "                          output_dim=EMBEDDING_DIM,\n",
    "                          weights = [embeddings_matrix], trainable= False, name='word_embedding_layer', \n",
    "                          mask_zero=True)) # trainable=True results in overfitting\n",
    "\n",
    "model_4.add(LSTM(LSTM_DIM, return_sequences=True, name='lstm_layer')) # Can try Bidirectional-LSTM\n",
    "model_4.add(Dropout(rate=0.2, name='dropout_1')) # Can try varying dropout rates, in paper suggest 0.2\n",
    "model_4.add(Activation(activation='relu', name='activation_1'))\n",
    "#output (batch_size, timesteps, input_dim)\n",
    "model_4.add(attention())\n",
    "model_4.add(Dense(4, activation='softmax', name='output_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "colab_type": "code",
    "id": "7DTWTp8KG-_z",
    "outputId": "532f9c76-a3c1-4e7c-c806-acc3622ac0bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_embedding_layer (Embedd (None, 700, 50)           1686400   \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 700, 50)           20200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 700, 50)           0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 700, 50)           0         \n",
      "_________________________________________________________________\n",
      "attention_1 (attention)      (None, 50)                750       \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 4)                 204       \n",
      "=================================================================\n",
      "Total params: 1,707,554\n",
      "Trainable params: 21,154\n",
      "Non-trainable params: 1,686,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQeVUjhGHV-K"
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(lr=0.001) # Try a different learning rate\n",
    "\n",
    "model_4.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "colab_type": "code",
    "id": "g8qHNUhUHdhe",
    "outputId": "cf6d85bf-c085-493f-be70-258007687b72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "71/71 [==============================] - 198s 3s/step - loss: 0.9084 - accuracy: 0.6916 - val_loss: 0.8012 - val_accuracy: 0.7299\n",
      "Epoch 2/20\n",
      "71/71 [==============================] - 197s 3s/step - loss: 0.7829 - accuracy: 0.7353 - val_loss: 0.7861 - val_accuracy: 0.7299\n",
      "Epoch 3/20\n",
      "71/71 [==============================] - 198s 3s/step - loss: 0.7676 - accuracy: 0.7355 - val_loss: 0.7731 - val_accuracy: 0.7306\n",
      "Epoch 4/20\n",
      "71/71 [==============================] - 198s 3s/step - loss: 0.7554 - accuracy: 0.7355 - val_loss: 0.7612 - val_accuracy: 0.7306\n",
      "Epoch 5/20\n",
      "71/71 [==============================] - 198s 3s/step - loss: 0.7434 - accuracy: 0.7363 - val_loss: 0.7571 - val_accuracy: 0.7326\n",
      "Epoch 6/20\n",
      "71/71 [==============================] - 199s 3s/step - loss: 0.7305 - accuracy: 0.7385 - val_loss: 0.7441 - val_accuracy: 0.7361\n",
      "Epoch 7/20\n",
      "71/71 [==============================] - 199s 3s/step - loss: 0.7423 - accuracy: 0.7376 - val_loss: 0.8017 - val_accuracy: 0.7323\n",
      "Epoch 8/20\n",
      "71/71 [==============================] - 198s 3s/step - loss: 0.7470 - accuracy: 0.7393 - val_loss: 0.7540 - val_accuracy: 0.7346\n",
      "Epoch 9/20\n",
      "71/71 [==============================] - 199s 3s/step - loss: 0.7292 - accuracy: 0.7425 - val_loss: 0.7368 - val_accuracy: 0.7435\n",
      "Epoch 10/20\n",
      "71/71 [==============================] - 198s 3s/step - loss: 0.7189 - accuracy: 0.7504 - val_loss: 0.7435 - val_accuracy: 0.7348\n",
      "Epoch 11/20\n",
      "71/71 [==============================] - 197s 3s/step - loss: 0.7132 - accuracy: 0.7526 - val_loss: 0.7259 - val_accuracy: 0.7358\n",
      "Epoch 12/20\n",
      "71/71 [==============================] - 197s 3s/step - loss: 0.6975 - accuracy: 0.7581 - val_loss: 0.7181 - val_accuracy: 0.7519\n",
      "Epoch 13/20\n",
      "71/71 [==============================] - 197s 3s/step - loss: 0.6988 - accuracy: 0.7538 - val_loss: 0.7007 - val_accuracy: 0.7546\n",
      "Epoch 14/20\n",
      "71/71 [==============================] - 198s 3s/step - loss: 0.6740 - accuracy: 0.7666 - val_loss: 0.6857 - val_accuracy: 0.7638\n",
      "Epoch 15/20\n",
      "71/71 [==============================] - 199s 3s/step - loss: 0.6579 - accuracy: 0.7756 - val_loss: 0.6763 - val_accuracy: 0.7703\n",
      "Epoch 16/20\n",
      "71/71 [==============================] - 197s 3s/step - loss: 0.6430 - accuracy: 0.7822 - val_loss: 0.6589 - val_accuracy: 0.7829\n",
      "Epoch 17/20\n",
      "71/71 [==============================] - 198s 3s/step - loss: 0.6303 - accuracy: 0.7873 - val_loss: 0.6607 - val_accuracy: 0.7802\n",
      "Epoch 18/20\n",
      "71/71 [==============================] - 199s 3s/step - loss: 0.6178 - accuracy: 0.7930 - val_loss: 0.6399 - val_accuracy: 0.7841\n",
      "Epoch 19/20\n",
      "71/71 [==============================] - 197s 3s/step - loss: 0.6040 - accuracy: 0.7976 - val_loss: 0.6351 - val_accuracy: 0.7844\n",
      "Epoch 20/20\n",
      "71/71 [==============================] - 197s 3s/step - loss: 0.5958 - accuracy: 0.7996 - val_loss: 0.6207 - val_accuracy: 0.7869\n"
     ]
    }
   ],
   "source": [
    "history_4 = model_4.fit(X_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=20,\n",
    "          validation_data=(X_vali, y_vali)\n",
    "         )\n",
    "model_4.save('lstm_with_attention_700.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sIx4NXWWHpx3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
